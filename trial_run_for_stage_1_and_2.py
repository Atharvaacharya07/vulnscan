# -*- coding: utf-8 -*-
"""trial run for stage 1 and 2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TWFYs7hW2qJJGRIKqLIPEsXFXhDvkZmy
"""

# ============================================
# HYBRID PIPELINE (Stage-1 IDS + Stage-2 Enhanced Vuln)
# ============================================

# ---- Imports ----
import os
import re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.metrics import classification_report
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline

# LightGBM with safe GPU->CPU fallback
from lightgbm import LGBMClassifier
from google.colab import drive

# ---------------------------------------------------
# Config: paths based on your Drive structure
# ---------------------------------------------------
DRIVE_ROOT = "/content/drive"
PROJECT_ROOT = f"{DRIVE_ROOT}/MyDrive/Dissertation_Project"
IDS_DIRS = {
    "CIC-IDS-2017": "cic_ids_2017",
    "CSE-CIC-IDS-2018": "cse_cic_ids_2018",
    "UNSW-NB15": "unsw_nb15",
}
DATASET_TO_RUN = "CIC-IDS-2017"  # <--- change here if you want a different IDS dataset

NESSUS_CSV = f"{PROJECT_ROOT}/simulated_nessus_data/findings_realistic_v3.csv"
ARTIFACTS_DIR = f"{PROJECT_ROOT}/artifacts_hybrid"
ALERTS_PATH = f"{ARTIFACTS_DIR}/alerts_stage1.csv"
ENRICHED_OUT = f"{ARTIFACTS_DIR}/vuln_prioritized_enriched.csv"
FEATURE_IMP_CSV = f"{ARTIFACTS_DIR}/feature_importances.csv"
RISK_BANDS_CSV = f"{ARTIFACTS_DIR}/risk_bands_summary.csv"
ASSET_INV_CSV = f"{PROJECT_ROOT}/simulated_nessus_data/asset_inventory.csv"  # optional
SHAP_PNG = f"{ARTIFACTS_DIR}/shap_summary.png"

# Stage-1 sampling to keep training fast; adjust if needed
STAGE1_SUBSAMPLE = 150_000
STAGE1_TEST_SIZE = 0.20

# Stage-2 training sample (large CSV); adjust higher if you have more horsepower
STAGE2_TRAIN_SAMPLE = 350_000
STAGE2_TEST_SIZE = 0.30

# ---------------------------------------------------
# Drive mount
# ---------------------------------------------------
drive.mount("/content/drive")

# ---------------------------------------------------
# Helpers
# ---------------------------------------------------
def _list_csvs(folder):
    return [os.path.join(folder, f) for f in os.listdir(folder) if f.lower().endswith(".csv")]

def _read_csv_any(path):
    try:
        return pd.read_csv(path, encoding="utf-8", low_memory=False)
    except UnicodeDecodeError:
        return pd.read_csv(path, encoding="latin1", low_memory=False)

def _fit_lightgbm_with_fallback(**kwargs):
    """
    Create an LGBMClassifier that tries GPU first, then CPU if GPU isn't available.
    """
    params = dict(
        random_state=42,
        n_estimators=300,
        learning_rate=0.1,
        n_jobs=-1,
    )
    params.update(kwargs or {})
    try:
        clf = LGBMClassifier(**{**params, "device": "gpu"})
        _ = clf.get_params()
        return clf
    except Exception:
        return LGBMClassifier(**params)

def _ensure_dir(path):
    os.makedirs(path, exist_ok=True)

# ===================================================
# Stage-1: IDS training + alerts export
# ===================================================
def stage1_run(dataset_name: str):
    print("\n=== Stage-1 (IDS) ===")
    dataset_dir = os.path.join(PROJECT_ROOT, IDS_DIRS[dataset_name])
    csvs = _list_csvs(dataset_dir)
    if not csvs:
        raise FileNotFoundError(f"No CSV files found under: {dataset_dir}")

    print("UTF-8 failed, trying latin1 encoding..." if any("Copy of" in os.path.basename(c) for c in csvs) else "")
    df = pd.concat((_read_csv_any(p) for p in csvs), ignore_index=True)
    df.columns = df.columns.str.strip()

    # Basic cleaning
    df.replace([np.inf, -np.inf], np.nan, inplace=True)
    df.dropna(inplace=True)

    # Label => is_attack
    if "Label" not in df.columns:
        raise ValueError("Expected 'Label' column in IDS data.")
    df["is_attack"] = (df["Label"].astype(str).str.strip() != "BENIGN").astype(int)

    print("[Stage-1] Label distribution:\n", df["is_attack"].value_counts())

    # Subsample for training speed
    if len(df) > STAGE1_SUBSAMPLE:
        _, df = train_test_split(
            df,
            test_size=STAGE1_SUBSAMPLE,
            random_state=42,
            stratify=df["is_attack"]
        )

    # Choose features: drop label + high-cardinality IDs; keep useful protocol/ports/flows
    drop_cols = {"Label", "is_attack", "Flow ID", "Timestamp", "Source IP", "Destination IP"}
    X = df.drop(columns=[c for c in drop_cols if c in df.columns], errors="ignore")
    y = df["is_attack"]

    # Split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=STAGE1_TEST_SIZE, random_state=42, stratify=y
    )

    # Numeric / categorical
    num_cols = X_train.select_dtypes(include=np.number).columns.tolist()
    cat_cols = X_train.select_dtypes(exclude=np.number).columns.tolist()

    preproc = ColumnTransformer(
        transformers=[
            ("num", StandardScaler(), num_cols),
            ("cat", OneHotEncoder(handle_unknown="ignore"), cat_cols),
        ],
        remainder="drop",
    )

    clf = _fit_lightgbm_with_fallback()

    pipe = ImbPipeline(steps=[
        ("pre", preproc),
        ("smote", SMOTE(random_state=42)),
        ("clf", clf),
    ])

    print("[Stage-1] Training...")
    pipe.fit(X_train, y_train)
    y_pred = pipe.predict(X_test)
    print("[Stage-1] Report:\n", classification_report(y_test, y_pred, target_names=["Benign", "Attack"]))

    # ---- Alerts export ----
    print("\n[Stage-1] Exporting alerts with assets=['Source IP','Destination IP'] and signatures=['Protocol','Destination Port']")
    _ensure_dir(ARTIFACTS_DIR)

    used_cols = [c for c in X.columns if c in df.columns]
    X_all = df[used_cols].copy()

    # Predict probabilities (triage generator)
    P = pipe.predict_proba(X_all)[:, 1]  # probability of 'Attack'
    df_alert = pd.DataFrame({"prob_attack": P})

    # Attach asset & signature columns if present
    for col in ["Source IP", "Destination IP", "Protocol", "Destination Port"]:
        if col in df.columns:
            df_alert[col] = df[col].values

    # Keep top-N alerts (use ~50k or fewer for reasonable size)
    keep_n = min(50_000, len(df_alert))
    df_alert = df_alert.sort_values("prob_attack", ascending=False).head(keep_n)

    df_alert.to_csv(ALERTS_PATH, index=False)
    print(f"[Stage-1] Alerts → {ALERTS_PATH}")

    return pipe

# ===================================================
# Patch Nessus CSV to add matching 'Source IP'
# ===================================================
def patch_nessus_with_source_ip():
    """
    Best fix: add a 'Source IP' column to the synthetic Nessus CSV
    sampled from Stage-1 alert IPs so Stage-2 can filter by attacked assets.
    """
    print("\n=== Patching Nessus CSV with 'Source IP' ===")
    if not os.path.exists(ALERTS_PATH):
        raise FileNotFoundError(f"Missing alerts file: {ALERTS_PATH}. Run Stage-1 first.")

    alerts = _read_csv_any(ALERTS_PATH)
    if "Source IP" not in alerts.columns and "Destination IP" not in alerts.columns:
        raise ValueError("Alerts CSV has no 'Source IP' or 'Destination IP'. Re-run Stage-1 with those columns present.")

    # Prefer Source IP; fallback to Destination IP
    asset_col = "Source IP" if "Source IP" in alerts.columns else "Destination IP"
    ips = alerts[asset_col].dropna().astype(str).str.strip()
    ips = ips[ips != ""].unique()
    if len(ips) == 0:
        raise ValueError("No non-empty IPs found in alerts! Cannot patch Nessus data.")

    nessus = _read_csv_any(NESSUS_CSV)

    # Create/replace 'Source IP' with a realistic distribution from alerts
    probs = alerts[asset_col].value_counts(normalize=True)
    prob_map = probs.to_dict()
    prob_vec = np.array([prob_map.get(ip, 0) for ip in ips], dtype=float)
    if prob_vec.sum() == 0:
        prob_vec = None  # fallback to uniform

    rng = np.random.default_rng(seed=42)
    nessus["Source IP"] = rng.choice(ips, size=len(nessus), p=(prob_vec / prob_vec.sum()) if prob_vec is not None else None)

    nessus.to_csv(NESSUS_CSV, index=False)
    print(f"[Patch] Added 'Source IP' to Nessus CSV → {NESSUS_CSV} (now matches Stage-1 alerts)")

# =====================================================================
# Stage-2 (ENHANCED): Vulnerability prioritisation + enrichment
# =====================================================================
# Enrichment helpers (keywords → TTP / remediation text)
TTP_MAPPING = {
    'T1110.001': ['brute force', 'ftp-patator', 'ssh-patator'],
    'T1498':     ['dos', 'ddos', 'hulk', 'goldeneye', 'slowloris', 'slowhttptest'],
    'T1190':     ['heartbleed'],
    'T1059.007': ['xss'],
    'T1505':     ['sql injection'],
    'T1071':     ['infiltration'],
    'T1583':     ['bot'],
    'T1046':     ['portscan'],
}
REMEDIATION_ADVICE = {
    'T1110.001': "Implement account lockout policies and MFA.",
    'T1498':     "Deploy WAF / DDoS mitigation and rate limiting.",
    'T1190':     "Patch the vulnerable public-facing application immediately.",
    'T1059.007': "Sanitize inputs; enable a WAF with XSS rules.",
    'T1505':     "Use parameterized queries and input validation.",
    'T1071':     "Monitor egress for unusual destinations; block C2.",
    'T1583':     "Harden perimeter; block known C2 IPs/domains.",
    'T1046':     "Throttle or block scanning sources; tighten ACLs.",
}
def map_ttp_and_fix(desc: str):
    if not isinstance(desc, str):
        return ("TTP Not Found", "No specific advice available.")
    s = desc.lower()
    found = "TTP Not Found"
    for ttp, kws in TTP_MAPPING.items():
        if any(k in s for k in kws):
            found = ttp
            break
    fix = REMEDIATION_ADVICE.get(found, "No specific advice available.")
    return (found, fix)

def stage2_run_enhanced():
    print("\n=== Stage-2 (Enhanced Vuln Prioritisation) ===")

    # 1) Load vulnerability findings
    dfv = _read_csv_any(NESSUS_CSV)
    print(f"[Stage-2] Loaded vulns: {dfv.shape}, cols={list(dfv.columns)[:12]}...")

    # Ensure an IP column exists (should exist after patch)
    if "Source IP" not in dfv.columns:
        # create deterministic synthetic IPs if missing
        unique_keys = pd.util.hash_pandas_object(dfv.get("description_len", pd.Series(range(len(dfv)))))
        dfv["Source IP"] = "10." + (unique_keys % 250).astype(str) + "." + ((unique_keys // 250) % 250).astype(str) + "." + ((unique_keys // 65536) % 250).astype(str)
        print("[Stage-2] 'Source IP' not found in findings; added synthetic IPs.")

    # 2) Load Stage-1 alerts and filter (focus on attacked assets)
    if os.path.exists(ALERTS_PATH):
        alerts = _read_csv_any(ALERTS_PATH)
        asset_cols = [c for c in ["Source IP","Destination IP"] if c in alerts.columns]
        alerted_ips = set()
        for c in asset_cols:
            alerted_ips |= set(alerts[c].dropna().astype(str).unique())
        before = len(dfv)
        dfv = dfv[dfv["Source IP"].astype(str).isin(alerted_ips)]
        print(f"[Stage-2] Asset filter via Stage-1 alerts applied: {before} → {len(dfv)}")
        if len(dfv) == 0:
            print("[Stage-2][WARN] No matching vulns for alerted assets. Proceeding on full dataset instead.")
            dfv = _read_csv_any(NESSUS_CSV)
    else:
        print("[Stage-2] Alerts CSV not found; proceeding on full dataset.")

    # 3) Optional: asset inventory join (criticality, exposure etc.)
    if os.path.exists(ASSET_INV_CSV):
        dfa = _read_csv_any(ASSET_INV_CSV)
        # Expect at least: Source IP, asset_criticality(1–5), internet_exposed(0/1)
        expected = ["Source IP", "asset_criticality", "internet_exposed"]
        for c in expected:
            if c not in dfa.columns:
                # create defaults if missing
                if c == "Source IP":
                    dfa[c] = []
                elif c == "asset_criticality":
                    dfa[c] = 3
                elif c == "internet_exposed":
                    dfa[c] = 0
        dfv = dfv.merge(dfa[["Source IP","asset_criticality","internet_exposed"]].drop_duplicates(),
                        on="Source IP", how="left")
    else:
        print("[Stage-2] No asset_inventory.csv. Using defaults.")
        dfv["asset_criticality"] = 3
        dfv["internet_exposed"] = 0

    # 4) Patch + threat-intel columns (create if missing)
    if "patch_available" not in dfv.columns:
        dfv["patch_available"] = (dfv["cvss"] >= 4.0).astype(int)
    if "patch_age_days" not in dfv.columns:
        dfv["patch_age_days"] = np.minimum(dfv["age_days"], (dfv["age_days"] * 0.6).astype(int))
    if "in_cisa_kev" not in dfv.columns:
        dfv["in_cisa_kev"] = ((dfv["severity"] >= 3) & (dfv["exploit_available"] == 1)).astype(int)
    if "has_known_exploit" not in dfv.columns:
        dfv["has_known_exploit"] = dfv["exploit_available"].astype(int)
    if "trending" not in dfv.columns:
        dfv["trending"] = ((dfv["age_days"] <= 45) & (dfv["severity"] >= 3)).astype(int)

    # 5) Simple NLP enrichment: build/ensure a text column and map to TTP + advice
    if "__desc" not in dfv.columns:
        dfv["__desc"] = (
            "service=" + dfv["svc_name"].astype(str) +
            " proto=" + dfv["proto"].astype(str) +
            " port=" + dfv["port"].astype(str)
        )
    dfv["ttp"], dfv["remediation_text"] = zip(*dfv["__desc"].map(map_ttp_and_fix))

    # 6) Train/test split and model (LightGBM)
    target = "remediation_priority"
    if target not in dfv.columns:
        raise ValueError(f"Missing target column '{target}' in {NESSUS_CSV}")

    # Feature set
    num_feats = ["cvss","severity","exploit_available","age_days","description_len","persistence_scans",
                 "port","asset_criticality","internet_exposed","patch_available","patch_age_days",
                 "in_cisa_kev","has_known_exploit","trending"]
    cat_feats = ["proto","svc_name"]
    keep_cols = ["Source IP", target] + num_feats + cat_feats + ["ttp","remediation_text"]

    dfm = dfv[keep_cols].dropna(subset=[target]).copy()
    if len(dfm) > STAGE2_TRAIN_SAMPLE:
        dfm = dfm.sample(STAGE2_TRAIN_SAMPLE, random_state=42)

    X = dfm[num_feats + cat_feats]
    y = dfm[target].astype(int)

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=STAGE2_TEST_SIZE, random_state=42, stratify=y
    )

    preproc = ColumnTransformer(
        transformers=[
            ("num", StandardScaler(), num_feats),
            ("cat", OneHotEncoder(handle_unknown="ignore"), cat_feats),
        ],
        remainder="drop"
    )

    clf = _fit_lightgbm_with_fallback(objective="multiclass", num_class=y.nunique())
    pipe = ImbPipeline(steps=[
        ("pre", preproc),
        ("smote", SMOTE(random_state=42)),
        ("clf", clf),
    ])

    print("[Stage-2] Training prioritisation model with context features...")
    pipe.fit(X_train, y_train)
    y_pred = pipe.predict(X_test)
    print("\n[Stage-2] Report:\n", classification_report(y_test, y_pred))

    # 7) Feature importances (from LightGBM)
    try:
        enc_feature_names = pipe.named_steps["pre"].get_feature_names_out()
        importances = pipe.named_steps["clf"].feature_importances_
        fi = (pd.DataFrame({"feature": enc_feature_names, "importance": importances})
                .sort_values("importance", ascending=False))
        _ensure_dir(ARTIFACTS_DIR)
        fi.to_csv(FEATURE_IMP_CSV, index=False)
        print(f"[Stage-2] Feature importances → {FEATURE_IMP_CSV}")
    except Exception as e:
        print(f"[Stage-2][WARN] Could not extract feature importances: {e}")

    # 8) Predict on the training set to build calibrated risk score
    proba = pipe.predict_proba(dfm[num_feats + cat_feats])  # shape: [N, classes]
    # Map classes to weights (higher class id => higher priority)
    cls_order = pipe.named_steps["clf"].classes_.tolist()
    weight_map = {c:(i+1) for i,c in enumerate(sorted(cls_order))}
    weights = np.array([weight_map[c] for c in cls_order], dtype=float)
    base_risk = (proba * weights).sum(axis=1) / weights.max()  # normalized ~[0,1]

    # Context multipliers
    crit = dfm["asset_criticality"].fillna(3).astype(float)
    exposed = dfm["internet_exposed"].fillna(0).astype(int)
    kev = dfm["in_cisa_kev"].fillna(0).astype(int)
    known = dfm["has_known_exploit"].fillna(0).astype(int)
    trend = dfm["trending"].fillna(0).astype(int)

    mult = 1.0
    mult *= (1 + 0.12 * (crit - 3))  # +/- ~24% across crit 1–5
    mult *= (1 + 0.10 * exposed)     # +10% if internet exposed
    mult *= (1 + 0.15 * kev)         # +15% if in KEV
    mult *= (1 + 0.10 * known)       # +10% if known exploit
    mult *= (1 + 0.08 * trend)       # +8% if trending

    risk_score = np.clip(base_risk * mult, 0, 1.0)

    # 9) Risk bands + SLA hints
    def band_sla(r):
        if r >= 0.80: return ("P1", "Patch within 24–48h")
        if r >= 0.60: return ("P2", "Patch within 7 days")
        if r >= 0.40: return ("P3", "Patch within 14 days")
        if r >= 0.20: return ("P4", "Patch within 30 days")
        return ("P5", "Monitor / scheduled patch")

    rb, sla = [], []
    for r in risk_score:
        b, s = band_sla(r)
        rb.append(b); sla.append(s)

    # 10) Build export dataframe (top columns first for readability)
    export_cols_head = [
        "Source IP", "svc_name", "proto", "port",
        "asset_criticality", "internet_exposed",
        "cvss","severity","exploit_available","age_days","patch_available","patch_age_days",
        "in_cisa_kev","has_known_exploit","trending",
        "remediation_priority",
        "ttp","remediation_text",
    ]
    export_df = dfm[export_cols_head].copy()
    export_df["predicted_priority"] = pipe.predict(dfm[num_feats + cat_feats])
    export_df["predicted_probs_max"] = proba.max(axis=1)
    export_df["risk_score"] = risk_score
    export_df["priority_band"] = rb
    export_df["sla_hint"] = sla

    # sort: highest risk first within band
    export_df = export_df.sort_values(["priority_band","risk_score","predicted_probs_max"],
                                      ascending=[True, False, False])

    _ensure_dir(ARTIFACTS_DIR)
    export_df.to_csv(ENRICHED_OUT, index=False)
    print(f"[Stage-2] Prioritised enrichment → {ENRICHED_OUT}")

    # 11) Risk band summary
    summary = (export_df.groupby("priority_band")
               .agg(count=("Source IP","count"),
                    mean_risk=("risk_score","mean"),
                    mean_cvss=("cvss","mean"))
               .reset_index()
               .sort_values("priority_band"))
    summary.to_csv(RISK_BANDS_CSV, index=False)
    print(f"[Stage-2] Risk band summary → {RISK_BANDS_CSV}")
    print(summary)

    # 12) Optional: SHAP explainability
    try:
        import shap
        shap_sample = dfm.sample(min(len(dfm), 5000), random_state=42)
        X_shap = pipe.named_steps["pre"].transform(shap_sample[num_feats + cat_feats])
        explainer = shap.TreeExplainer(pipe.named_steps["clf"])
        shap_values = explainer.shap_values(X_shap)
        shap.summary_plot(shap_values, X_shap,
                          feature_names=pipe.named_steps["pre"].get_feature_names_out(),
                          show=False)
        plt.tight_layout()
        plt.savefig(SHAP_PNG, dpi=150)
        plt.close()
        print(f"[Stage-2] SHAP summary plot → {SHAP_PNG}")
    except Exception as e:
        print(f"[Stage-2] SHAP not generated (optional): {e}")

    print("\n✅ Stage-2 (Enhanced) complete.")
    return pipe

# ============================================
# Run both stages, with the best-fix patch
# ============================================
_ = stage1_run(DATASET_TO_RUN)
patch_nessus_with_source_ip()     # ensure Nessus CSV has matching 'Source IP'
_ = stage2_run_enhanced()

print("\n✅ Done. Artifacts:")
print(f"  - Stage-1 alerts: {ALERTS_PATH}")
print(f"  - Stage-2 prioritised: {ENRICHED_OUT}")
print(f"  - Feature importances: {FEATURE_IMP_CSV}")
print(f"  - Risk bands summary: {RISK_BANDS_CSV}")
print(f"  - (Optional) SHAP plot: {SHAP_PNG}")